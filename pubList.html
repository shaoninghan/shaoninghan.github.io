<!-- Generated from JabRef by PubList by Truong Nghiem at 11:51 on 2024.08.27. -->
<ol class="biblist">
<!-- Item: ghl2024realtime -->
<li ><p>
Andr&eacute;s G&oacute;mez, Shaoning Han and Leonardo Lozano,
&ldquo;Real-Time Solution of Quadratic Optimization Problems with Banded Matrices and Indicator Variables,&rdquo;
<i>Submitted, </i>




2024.


<br />
<a href="javascript:toggleAbstract('ghl2024realtime')">[abstract]</a>
<a href="javascript:toggleBibtex('ghl2024realtime')">[bibtex]</a>
<a href="https://arxiv.org/abs/2405.03051">[url]</a>





</p>
<div id="abstract_ghl2024realtime" class="abstract noshow">
We consider mixed-integer quadratic optimization problems with banded matrices and indicator variables. These problems arise pervasively in statistical inference problems with time-series data, where the banded matrix captures the temporal relationship of the underlying process. In particular, the problem studied arises in monitoring problems, where the decision-maker wants to detect changes or anomalies. We propose to solve these problems using decision diagrams. In particular we show how to exploit the temporal dependencies to construct diagrams with size polynomial in the number of decision variables. We also describe how to construct the convex hull of the set under study from the decision diagrams, and how to deploy the method online to solve the problems in milliseconds via a shortest path algorithm.
</div>
<div id="bib_ghl2024realtime" class="bibtex noshow">
<pre>
@article{ghl2024realtime,
  author = {G&oacute;mez, Andr&eacute;s and Han, Shaoning and Lozano, Leonardo},
  title = {Real-Time Solution of Quadratic Optimization Problems with Banded Matrices and Indicator Variables},
  year = {2024},
  url = {https://arxiv.org/abs/2405.03051}
}
</pre>
</div>
</li>
<!-- Item: cgh2024robust -->
<li ><p>
Valentina Cepeda, Andr&eacute;s G&oacute;mez and Shaoning Han,
&ldquo;Robust Support Vector Machines via Conic Optimization,&rdquo;
<i>Submitted, </i>




2024.


<br />
<a href="javascript:toggleAbstract('cgh2024robust')">[abstract]</a>
<a href="javascript:toggleBibtex('cgh2024robust')">[bibtex]</a>
<a href="https://optimization-online.org/2024/02/robust-support-vector-machines-via-conic-optimization">[url]</a>





</p>
<div id="abstract_cgh2024robust" class="abstract noshow">
We consider the problem of learning support vector machines robust to uncertainty. It has been established in the literature that typical loss functions, including the hinge loss, are sensible to data perturbations and outliers, thus performing poorly in the setting considered. In contrast, using the 0-1 loss or a suitable non-convex approximation results in robust estimators, at the expense of large computational costs. In this paper we use mixed-integer optimization techniques to derive a new loss function that better approximates the 0-1 loss compared with existing alternatives, while preserving the convexity of the learning problem. In our computational results, we show that the proposed estimator is competitive with the standard SVMs with the hinge loss in outlier-free regimes and better in the presence of outliers.
</div>
<div id="bib_cgh2024robust" class="bibtex noshow">
<pre>
@article{cgh2024robust,
  author = {Cepeda, Valentina and G&oacute;mez, Andr&eacute;s and Han, Shaoning},
  title = {Robust Support Vector Machines via Conic Optimization},
  year = {2024},
  url = {https://optimization-online.org/2024/02/robust-support-vector-machines-via-conic-optimization}
}
</pre>
</div>
</li>
<!-- Item: hcp2023nonlsc -->
<li ><p>
Shaoning Han, Ying Cui and Jong-Shi Pang,
&ldquo;Analysis of a Class of Minimization Problems Lacking Lower Semicontinuity,&rdquo;
<i> <b>Mathematics of Operations Research</b>,</i>




2024.


<br />
<a href="javascript:toggleAbstract('hcp2023nonlsc')">[abstract]</a>
<a href="javascript:toggleBibtex('hcp2023nonlsc')">[bibtex]</a>
<a href="https://pubsonline.informs.org/doi/abs/10.1287/moor.2023.0295">[url]</a>





</p>
<div id="abstract_hcp2023nonlsc" class="abstract noshow">
The minimization of non-lower semicontinuous functions is a difficult topic that has been minimally studied.  Among such functions is a Heaviside composite function that is the composition of a Heaviside function with a possibly nonsmooth multivariate function.  Unifying a statistical estimation problem with hierarchical selection of variables and a sample average approximation of composite chance constrained stochastic programs, a Heaviside composite optimization problem is one whose objective and constraints are defined by sums of possibly nonlinear multiples of such composite functions. Via a pulled-out formulation, a pseudo stationarity concept for a feasible point was introduced in an earlier work as a necessary condition for a local minimizer of a Heaviside composite optimization problem.  The present paper extends this previous study in several directions: (a) showing that pseudo stationarity is implied by, thus weaker than, a sharper subdifferential based stationarity condition which we term epi-stationarity; (b) introducing a set-theoretic sufficient condition, which we term local convexity-like property, under which an epi-stationary point of a possibly non-lower semicontinuous optimization problem is a local minimizer; (c) providing several classes of Heaviside composite functions satisfying this local convexity-like property; (d) extending the epigraphical formulation of a nonnegative multiple of a Heaviside composite function to a lifted formulation for arbitrarily signed multiples of the Heaviside composite function, based on which we show that an epi-stationary solution of the given Heaviside composite program with broad classes of B-differentiable component functions can in principle be approximately computed by surrogation methods.
</div>
<div id="bib_hcp2023nonlsc" class="bibtex noshow">
<pre>
@article{hcp2023nonlsc,
  author = {Han, Shaoning and Cui, Ying and Pang, Jong-Shi},
  title = {Analysis of a Class of Minimization Problems Lacking Lower Semicontinuity},
  journal = { Mathematics of Operations Research},
  year = {2024},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/moor.2023.0295}
}
</pre>
</div>
</li>
<!-- Item: hzp2023simplex -->
<li ><p>
Shaoning Han, Xinyao Zhang and Jong-Shi Pang,
&ldquo;On the Number of Pivots of Dantzig's Simplex Methods for Linear and Convex Quadratic Programs,&rdquo;
<i> <b>Operations Research Letters</b>,</i>




2024.


<br />
<a href="javascript:toggleAbstract('hzp2023simplex')">[abstract]</a>
<a href="javascript:toggleBibtex('hzp2023simplex')">[bibtex]</a>
<a href="https://www.sciencedirect.com/science/article/pii/S0167637724000270">[url]</a>

<a href="slides/ios2024Xinyao.pdf">[slides]</a>



</p>
<div id="abstract_hzp2023simplex" class="abstract noshow">
Refining and extending works by Ye and Kitahara-Mizuno, this paper presents new results on the number of pivots of simplex-type methods for solving linear programs of the Leontief kind, certain linear complementarity problems of the P kind, and nonnegatively constrained convex quadratic programs. Our results contribute to the further understanding of the complexity and efficiency of simplex-type methods for solving these problems. Two applications of the quadratic programming results are presented.
</div>
<div id="bib_hzp2023simplex" class="bibtex noshow">
<pre>
@article{hzp2023simplex,
  author = {Han, Shaoning and Zhang, Xinyao and Pang, Jong-Shi},
  title = {On the Number of Pivots of Dantzig's Simplex Methods for Linear and Convex Quadratic Programs},
  journal = { Operations Research Letters},
  year = {2024},
  url = {https://www.sciencedirect.com/science/article/pii/S0167637724000270},
  doi = {https://doi.org/10.1016/j.orl.2024.107091}
}
</pre>
</div>
</li>
<!-- Item: han2022polynomial -->
<li ><p>
Shaoning Han, Andr&eacute;s G&oacute;mez and Jong-Shi Pang,
&ldquo;On Polynomial-Time Solvability of Combinatorial Markov Random Fields,&rdquo;
<i>Submitted, </i>




2022.


<br />
<a href="javascript:toggleAbstract('han2022polynomial')">[abstract]</a>
<a href="javascript:toggleBibtex('han2022polynomial')">[bibtex]</a>
<a href="https://arxiv.org/abs/2209.13161">[url]</a>

<a href="slides/ios2024.pdf">[slides]</a>



</p>
<div id="abstract_han2022polynomial" class="abstract noshow">
The problem of inferring Markov random fields (MRFs) with a sparsity or robustness prior can be naturally modeled as a mixed-integer program. This motivates us to study a general class of convex submodular optimization problems with indicator variables, which we show to be polynomially solvable in this paper. The key insight is that, possibly after a suitable reformulation, indicator constraints preserve submodularity. Fast computations of the associated Lov&aacute;sz extensions are also discussed under certain smoothness conditions, and can be implemented using only linear-algebraic operations in the case of quadratic objectives.
</div>
<div id="bib_han2022polynomial" class="bibtex noshow">
<pre>
@article{han2022polynomial,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s and Pang, Jong-Shi},
  title = {On Polynomial-Time Solvability of Combinatorial Markov Random Fields},
  year = {2022},
  url = {https://arxiv.org/abs/2209.13161}
}
</pre>
</div>
</li>
<!-- Item: han2022continuous -->
<li ><p>
Shaoning Han and Jong-Shi Pang,
&ldquo;Continuous Selections of Solutions to Parametric Variational Inequalities,&rdquo;
<i> <b>SIAM Journal on Optimization</b>,</i>
vol. 34,
no. 1,
pp. 870-892,

2024.


<br />
<a href="javascript:toggleAbstract('han2022continuous')">[abstract]</a>
<a href="javascript:toggleBibtex('han2022continuous')">[bibtex]</a>
<a href="https://doi.org/10.1137/22M1514982">[url]</a>





</p>
<div id="abstract_han2022continuous" class="abstract noshow">
Abstract. This paper studies the existence of a (Lipschitz) continuous (single-valued) solution function of parametric variational inequalities under functional and constraint perturbations. At the most elementary level, this issue can be explained from classical parametric linear programming and its resolution by the parametric simplex method, which computes a solution trajectory of the problem when the objective coefficients and the right-hand sides of the constraints are parameterized by a single scalar parameter. The computed optimal solution vector (and not the optimal objective value) is a continuous piecewise affine function in the parameter when the objective coefficients are kept constant, whereas the computed solution vector can be discontinuous when the right-hand constraint coefficients are kept fixed and there is a basis change at a critical value of the parameter in the objective. We investigate this issue more broadly first in the context of an affine variational inequality (AVI) and obtain results that go beyond those pertaining to the lower semicontinuity of the solution map with joint vector perturbations; the latter property is closely tied to a stability theory of a parametric AVI and in particular to Robinson’s seminal concept of strong regularity. Extensions to nonlinear variational inequalities is also investigated without requiring solution uniqueness (and therefore applicable to nonstrongly regular problems). The role of solution uniqueness in this issue of continuous single-valued solution selection is further clarified.
</div>
<div id="bib_han2022continuous" class="bibtex noshow">
<pre>
@article{han2022continuous,
  author = {Han, Shaoning and Pang, Jong-Shi},
  title = {Continuous Selections of Solutions to Parametric Variational Inequalities},
  journal = { SIAM Journal on Optimization},
  year = {2024},
  volume = {34},
  number = {1},
  pages = {870-892},
  url = {https://doi.org/10.1137/22M1514982},
  doi = {https://doi.org/10.1137/22M1514982}
}
</pre>
</div>
</li>
<!-- Item: pang2023some -->
<li ><p>
Jong-Shi Pang and Shaoning Han,
&ldquo;Some Strongly Polynomially Solvable Convex Quadratic Programs with Bounded Variables,&rdquo;
<i> <b>SIAM Journal on Optimization</b>,</i>
vol. 33,
no. 2,
pp. 899-920,

2023.


<br />
<a href="javascript:toggleAbstract('pang2023some')">[abstract]</a>
<a href="javascript:toggleBibtex('pang2023some')">[bibtex]</a>
<a href="https://doi.org/10.1137/21M1463793">[url]</a>

<a href="slides/ios2022.pdf">[slides]</a>



</p>
<div id="abstract_pang2023some" class="abstract noshow">
This paper begins with a class of convex quadratic programs (QPs) with bounded variables solvable by the parametric principal pivoting algorithm with &Oscr;<span style="font-size: 1.4em;">(</span>n<sup>3</sup><span style="font-size: 1.4em;">)</span> strongly polynomial complexity, where n is the number of variables of the problem. Extensions of this Hessian class are the main contributions of this paper, which is motivated by a recent paper [P. Liu, S. Fattahi, A. Gómez, and S. Küçükyavuz, Math. Program. (2022), https://doi.org/10.1007/s10107-022-01845-0], wherein the efficient solution of a quadratic program with a tridiagonal Hessian matrix in the quadratic objective is needed for the construction of a polynomial-time algorithm for solving an associated sparse variable selection problem. With the tridiagonal structure, the complexity of the QP algorithm reduces to &Oscr;<span style="font-size: 1.4em;">(</span>n<sup>2</sup><span style="font-size: 1.4em;">)</span>. Our strongly polynomiality results extend previous works of some strongly polynomially solvable linear complementarity problems with a P-matrix [J. S. Pang and R. Chandrasekaran, Math. Program. Stud., 25 (1985), pp. 13–27]; special cases of the extended results include weakly quasi-diagonally dominant problems in addition to the tridiagonal ones.
</div>
<div id="bib_pang2023some" class="bibtex noshow">
<pre>
@article{pang2023some,
  author = {Pang, Jong-Shi and Han, Shaoning},
  title = {Some Strongly Polynomially Solvable Convex Quadratic Programs with Bounded Variables},
  journal = { SIAM Journal on Optimization},
  year = {2023},
  volume = {33},
  number = {2},
  pages = {899-920},
  url = {https://doi.org/10.1137/21M1463793},
  doi = {https://doi.org/10.1137/21M1463793}
}
</pre>
</div>
</li>
<!-- Item: han2021equiv -->
<li ><p>
Shaoning Han, Andr&eacute;s G&oacute;mez and Alper Atamt&uuml;rk,
&ldquo;The Equivalence of Optimal Perspective Formulation and Shor's SDP for Quadratic Programs with Indicator Variables,&rdquo;
<i> <b>Operations Research Letters</b>,</i>
vol. 50,
no. 2,
pp. 195-198,

2022.


<br />
<a href="javascript:toggleAbstract('han2021equiv')">[abstract]</a>
<a href="javascript:toggleBibtex('han2021equiv')">[bibtex]</a>
<a href="https://www.sciencedirect.com/science/article/pii/S0167637722000141">[url]</a>





</p>
<div id="abstract_han2021equiv" class="abstract noshow">
In this paper, we compare the strength of the optimal perspective reformulation and Shor's SDP relaxation. We prove these two formulations are equivalent for quadratic optimization problems with indicator variables.
</div>
<div id="bib_han2021equiv" class="bibtex noshow">
<pre>
@article{han2021equiv,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s and Atamt&uuml;rk, Alper},
  title = {The Equivalence of Optimal Perspective Formulation and Shor's SDP for Quadratic Programs with Indicator Variables},
  journal = { Operations Research Letters},
  year = {2022},
  volume = {50},
  number = {2},
  pages = {195-198},
  url = {https://www.sciencedirect.com/science/article/pii/S0167637722000141},
  doi = {https://doi.org/10.1016/j.orl.2022.01.007}
}
</pre>
</div>
</li>
<!-- Item: han2021compact -->
<li ><p>
Shaoning Han and Andr&eacute;s G&oacute;mez,
&ldquo;Compact Extended Formulations for Low-Rank Functions with Indicator Variables,&rdquo;
<i> <b>Mathematics of Operations Research</b>,</i>




2024.


<br />
<a href="javascript:toggleAbstract('han2021compact')">[abstract]</a>
<a href="javascript:toggleBibtex('han2021compact')">[bibtex]</a>
<a href="https://pubsonline.informs.org/doi/abs/10.1287/moor.2021.0281">[url]</a>

<a href="slides/informs2023-low-rank.pdf">[slides]</a>


<br /><b>&#x2022; Third Place, INFORMS Junior Faculty Interest Group (JFIG) Paper Competition, 2023</b> 
</p>
<div id="abstract_han2021compact" class="abstract noshow">
We study the mixed-integer epigraph of a special class of convex functions with nonconvex indicator constraints, which are often used to impose logical constraints on the support of the solutions. The class of functions we consider are defined as compositions of low-dimensional nonlinear functions with affine functions. Extended formulations describing the convex hull of such sets can easily be constructed via disjunctive programming although a direct application of this method often yields prohibitively large formulations, whose size is exponential in the number of variables. In this paper, we propose a new disjunctive representation of the sets under study, which leads to compact formulations with size exponential in the dimension of the nonlinear function but polynomial in the number of variables. Moreover, we show how to project out the additional variables for the case of dimension one, recovering or generalizing known results for the convex hulls of such sets (in the original space of variables). Our computational results indicate that the proposed approach can significantly improve the performance of solvers in structured problems.
</div>
<div id="bib_han2021compact" class="bibtex noshow">
<pre>
@article{han2021compact,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s},
  title = {Compact Extended Formulations for Low-Rank Functions with Indicator Variables},
  journal = { Mathematics of Operations Research},
  year = {2024},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/moor.2021.0281},
  doi = {https://doi.org/10.1287/moor.2021.0281}
}
</pre>
</div>
</li>
<!-- Item: he2023comparing -->
<li ><p>
Ziyu He, Shaoning Han, Andr&eacute;s G&oacute;mez, Ying Cui and Jong-Shi Pang,
&ldquo;Comparing Solution Paths of Sparse Quadratic Minimization with a Stieltjes Matrix,&rdquo;
<i> <b>Mathematical Programming</b>,</i>
vol. 204,

pp. 517-566,

2024.


<br />
<a href="javascript:toggleAbstract('he2023comparing')">[abstract]</a>
<a href="javascript:toggleBibtex('he2023comparing')">[bibtex]</a>
<a href="https://doi.org/10.1007/s10107-023-01966-0">[url]</a>

<a href="slides/solutionPathZiyu.pdf">[slides]</a>



</p>
<div id="abstract_he2023comparing" class="abstract noshow">
This paper studies several solution paths of sparse quadratic minimization problems as a function of the weighing parameter of the bi-objective of estimation loss versus solution sparsity. Three such paths are considered: the "&ell;<sub>0</sub>-path" where the discontinuous &ell;<sub>0</sub>-function provides the exact sparsity count; the "&ell;<sub>1</sub>-path" where the &ell;<sub>1</sub>-function provides a convex surrogate of sparsity count; and the "capped &ell;<sub>1</sub>-path" where the nonconvex nondifferentiable capped &ell;<sub>1</sub>-function aims to enhance the &ell;<sub>1</sub>-approximation. Serving different purposes, each of these three formulations is different from each other, both analytically and computationally. Our results deepen the understanding of (old and new)properties of the associated paths, highlight the pros, cons, and tradeoffs of these sparse optimization models, and provide numerical evidence to support the practical superiority of the capped &ell;<sub>1</sub>-path. Our study of the capped &ell;<sub>1</sub>-path is the first of its kind as the path pertains to computable directionally stationary (= strongly locally minimizing in this context, as opposed to globally optimal) solutions of a parametric nonconvex nondifferentiable optimization problem. Motivated by classical parametric quadratic programming theory and reinforced by modern statistical learning studies, both casting an exponential perspective in fully describing such solution paths, we also aim to address the question of whether some of them can be fully traced in strongly polynomial time in the problem dimensions. A major conclusion of this paper is that a path of directional stationary solutions of the capped &ell;<sub>1</sub>-regularized problem offers interesting theoretical properties and practical compromise between the &ell;<sub>0</sub>-path and the &ell;<sub>0</sub>-path. Indeed, while the &ell;<sub>0</sub>-path is computationally prohibitive and greatly handicapped by the repeated solution of mixed integer nonlinear programs, the quality of &ell;<sub>1</sub>-path, in terms of the two criteria---loss and sparsity--- in the estimation objective, is inferior to the capped &ell;<sub>1</sub>-path; the latter can be obtained efficiently by a combination of a parametric pivoting-like scheme supplemented by an algorithm that takes advantage of the Z-matrix structure of the loss function.
</div>
<div id="bib_he2023comparing" class="bibtex noshow">
<pre>
@article{he2023comparing,
  author = {He, Ziyu and Han, Shaoning and G&oacute;mez, Andr&eacute;s and Cui, Ying and Pang, Jong-Shi},
  title = {Comparing Solution Paths of Sparse Quadratic Minimization with a Stieltjes Matrix},
  journal = { Mathematical Programming},
  year = {2024},
  volume = {204},
  pages = {517--566},
  url = {https://doi.org/10.1007/s10107-023-01966-0},
  doi = {https://doi.org/10.1007/s10107-023-01966-0}
}
</pre>
</div>
</li>
<!-- Item: han2021single -->
<li ><p>
Shaoning Han and Andr&eacute;s G&oacute;mez,
&ldquo;Single-Neuron Convexifications for Binarized Neural Networks,&rdquo;
<i>Technical Report, </i>




2021.


<br />
<a href="javascript:toggleAbstract('han2021single')">[abstract]</a>
<a href="javascript:toggleBibtex('han2021single')">[bibtex]</a>
<a href="http://www.optimization-online.org/DB_HTML/2021/05/8419.html">[url]</a>





</p>
<div id="abstract_han2021single" class="abstract noshow">
Binarized neural networks are an important class of neural network in deep learning due to their computational efficiency. This paper contributes towards a better understanding of the structure of binarized neural networks, specifically, ideal convex representations of the activation functions used. We describe the convex hull of the graph of the signum activation function associated with a single neuron, deriving closed forms for the convex and concave envelopes that improve upon those used in the literature. The new formulations lead to improved methods to verify the robustness of a binarized neural network via convex optimization.
</div>
<div id="bib_han2021single" class="bibtex noshow">
<pre>
@article{han2021single,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s},
  title = {Single-Neuron Convexifications for Binarized Neural Networks},
  year = {2021},
  url = {http://www.optimization-online.org/DB_HTML/2021/05/8419.html}
}
</pre>
</div>
</li>
<!-- Item: HGA2023 -->
<li ><p>
Shaoning Han, Andr&eacute;s G&oacute;mez and Alper Atamt&uuml;rk,
&ldquo;2&times;2-Convexifications For Convex Quadratic Optimization with Indicator Variables,&rdquo;
<i> <b>Mathematical Programming</b>,</i>
vol. 202,

pp. 95-134,

2023.


<br />
<a href="javascript:toggleAbstract('HGA2023')">[abstract]</a>
<a href="javascript:toggleBibtex('HGA2023')">[bibtex]</a>
<a href="https://doi.org/10.1007/s10107-023-01924-w">[url]</a>

<a href="slides/informs2020.pdf">[slides]</a>



</p>
<div id="abstract_HGA2023" class="abstract noshow">
In this paper, we study the convex quadratic optimization problem with indicator variables. For the 2&times;2 case, we describe the convex hull of the epigraph in the original space of variables, and also give a conic quadratic extended formulation. Then, using the convex hull description for the 2&times;2 case as a building block, we derive an extended SDP relaxation for the general case. This new formulation is stronger than other SDP relaxations proposed in the literature for the problem, including the optimal perspective relaxation and the optimal rank-one relaxation. Computational experiments indicate that the proposed formulations are quite effective in reducing the integrality gap of the optimization problems.
</div>
<div id="bib_HGA2023" class="bibtex noshow">
<pre>
@article{HGA2023,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s and Atamt&uuml;rk, Alper},
  title = {2&times;2-Convexifications For Convex Quadratic Optimization with Indicator Variables},
  journal = { Mathematical Programming},
  year = {2023},
  volume = {202},
  pages = {95--134},
  url = {https://doi.org/10.1007/s10107-023-01924-w},
  doi = {https://doi.org/10.1007/s10107-023-01924-w}
}
</pre>
</div>
</li>
<!-- Item: han2019fractional -->
<li ><p>
Shaoning Han, Andr&eacute;s G&oacute;mez and Oleg A. Prokopyev,
&ldquo;Fractional 0-1 Programming and Submodularity,&rdquo;
<i> <b>Journal of Global Optimization</b>,</i>
vol. 84,

pp. 77-93,

2022.


<br />
<a href="javascript:toggleAbstract('han2019fractional')">[abstract]</a>
<a href="javascript:toggleBibtex('han2019fractional')">[bibtex]</a>
<a href="https://doi.org/10.1007/s10898-022-01131-5">[url]</a>

<a href="slides/informs2021.pdf">[slides]</a>


<br /><b>&#x2022; Honorable Mention, Journal of Global Optimization Best Paper Award, 2023</b> 
</p>
<div id="abstract_han2019fractional" class="abstract noshow">
In this note we study multiple-ratio fractional 0--1 programs, a broad class of NP-hard combinatorial optimization problems. In particular, under some relatively mild assumptions we provide a complete characterization of the conditions, which ensure that a single-ratio function is submodular. Then we illustrate our theoretical results with the assortment optimization and facility location problems, and discuss practical situations that guarantee submodularity in the considered application settings. In such cases, near-optimal solutions for multiple-ratio fractional 0--1 programs can be found via simple greedy algorithms.
</div>
<div id="bib_han2019fractional" class="bibtex noshow">
<pre>
@article{han2019fractional,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s and Prokopyev, Oleg A},
  title = {Fractional 0-1 Programming and Submodularity},
  journal = { Journal of Global Optimization},
  year = {2022},
  volume = {84},
  pages = {77--93},
  url = {https://doi.org/10.1007/s10898-022-01131-5}
}
</pre>
</div>
</li>
<!-- Item: atamturk2021sparse -->
<li ><p>
Alper Atamt&uuml;rk, Andr&eacute;s G&oacute;mez and Shaoning Han,
&ldquo;Sparse and Smooth Signal Estimation: Convexification of &ell;<sub>0</sub>-Formulations,&rdquo;
<i> <b>Journal of Machine Learning Research</b>,</i>
vol. 22,
no. 52,
pp. 1-43,

2021.
[alphabetical order]

<br />
<a href="javascript:toggleAbstract('atamturk2021sparse')">[abstract]</a>
<a href="javascript:toggleBibtex('atamturk2021sparse')">[bibtex]</a>
<a href="https://www.jmlr.org/papers/volume22/18-745/18-745.pdf">[url]</a>





</p>
<div id="abstract_atamturk2021sparse" class="abstract noshow">
Signal estimation problems with smoothness and sparsity priors can be naturally modeled as quadratic optimization with &ell;<sub>0</sub>-``norm" constraints. Since such problems are non-convex and hard-to-solve, the standard approach is, instead, to tackle their convex surrogates based on &ell;<sub>1</sub>-norm relaxations. In this paper, we propose new iterative conic quadratic relaxations that exploit not only the &ell;<sub>0</sub>-``norm" terms, but also the fitness and smoothness functions. The iterative convexification approach substantially closes the gap between the &ell;<sub>0</sub>-``norm" and its &ell;<sub>1</sub> surrogate. Experiments using an off-the-shelf conic quadratic solver on synthetic as well as real datasets indicate that the proposed iterative convex relaxations lead to significantly better estimators than &ell;<sub>1</sub>-norm while preserving the computational efficiency. In addition, the parameters of the model and the resulting estimators are easily interpretable.
</div>
<div id="bib_atamturk2021sparse" class="bibtex noshow">
<pre>
@article{atamturk2021sparse,
  author = {Atamt&uuml;rk, Alper and G&oacute;mez, Andr&eacute;s and Han, Shaoning},
  title = {Sparse and Smooth Signal Estimation: Convexification of &ell;<sub>0</sub>-Formulations},
  journal = { Journal of Machine Learning Research},
  year = {2021},
  volume = {22},
  number = {52},
  pages = {1--43},
  url = {https://www.jmlr.org/papers/volume22/18-745/18-745.pdf}
}
</pre>
</div>
</li>
</ol>
