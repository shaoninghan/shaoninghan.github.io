<<<<<<< HEAD
<!-- Generated from JabRef by PubList by Truong Nghiem at 10:08 on 2021.06.23. -->
<ol class="biblist">
<!-- Item: han2021single -->
<li ><p>
Shaoning Han and Andr&eacute;s G&oacute;mez,
&ldquo;Single-neuron convexifications for binarized neural networks,&rdquo;





2021.

<br />
<a href="javascript:toggleAbstract('han2021single')">[abstract]</a>
<a href="javascript:toggleBibtex('han2021single')">[bibtex]</a>
<a href="http://www.optimization-online.org/DB_HTML/2021/05/8419.html">[url]</a>




</p>
<div id="abstract_han2021single" class="abstract">
Binarized neural networks are an important class of neural network in deep learning due to their computational efficiency. This paper contributes towards a better understanding of the structure of binarized neural networks, specifically, ideal convex representations of the activation functions used. We describe the convex hull of the graph of the signum activation function associated with a single neuron, deriving closed forms for the convex and concave envelopes that improve upon those used in the literature. The new formulations lead to improved methods to verify the robustness of a binarized neural network via convex optimization.
</div>
<div id="bib_han2021single" class="bibtex noshow">
<pre>
@article{han2021single,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s},
  title = {Single-neuron convexifications for binarized neural networks},
  year = {2021},
  url = {http://www.optimization-online.org/DB_HTML/2021/05/8419.html}
}
</pre>
</div>
</li>
<!-- Item: han20202x2 -->
<li ><p>
Shaoning Han, Andr&eacute;s G&oacute;mez and Alper Atamt&uuml;rk,
&ldquo;2x2 convexifications for convex quadratic optimization with indicator variables,&rdquo;
<i>arXiv preprint arXiv:2004.07448</i>,




2020.

<br />
<a href="javascript:toggleAbstract('han20202x2')">[abstract]</a>
<a href="javascript:toggleBibtex('han20202x2')">[bibtex]</a>
<a href="https://arxiv.org/abs/2004.07448">[url]</a>

<a href="slides/informs2020.pdf">[slide]</a>


</p>
<div id="abstract_han20202x2" class="abstract">
In this paper, we study the convex quadratic optimization problem with indicator variables. For the bivariate case, we describe the convex hull of the epigraph in the original space of variables, and also give a conic quadratic extended formulation. Then, using the convex hull description for the bivariate case as a building block, we derive an extended SDP relaxation for the general case. This new formulation is stronger than other SDP relaxations proposed in the literature for the problem, including Shor's SDP relaxation, the optimal perspective relaxation as well as the optimal rank-one relaxation. Computational experiments indicate that the proposed formulations are quite effective in reducing the integrality gap of the optimization problems.
</div>
<div id="bib_han20202x2" class="bibtex noshow">
<pre>
@article{han20202x2,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s and Atamt&uuml;rk, Alper},
  title = {2x2 convexifications for convex quadratic optimization with indicator variables},
  journal = {arXiv preprint arXiv:2004.07448},
  year = {2020},
  url = {https://arxiv.org/abs/2004.07448}
}
</pre>
</div>
</li>
<!-- Item: han2019fractional -->
<li ><p>
Shaoning Han, Andr&eacute;s G&oacute;mez and Oleg A. Prokopyev,
&ldquo;Fractional 0-1 programming and submodularity,&rdquo;
<i>arXiv preprint arXiv:2012.07235</i>,




2019.

<br />
<a href="javascript:toggleAbstract('han2019fractional')">[abstract]</a>
<a href="javascript:toggleBibtex('han2019fractional')">[bibtex]</a>
<a href="https://arxiv.org/abs/2012.07235">[url]</a>




</p>
<div id="abstract_han2019fractional" class="abstract">
In this note we study multiple-ratio fractional 0--1 programs, a broad class of NP-hard combinatorial optimization problems. In particular, under some relatively mild assumptions we provide a complete characterization of the conditions, which ensure that a single-ratio function is submodular. Then we illustrate our theoretical results with the assortment optimization and facility location problems, and discuss practical situations that guarantee submodularity in the considered application settings. In such cases, near-optimal solutions for multiple-ratio fractional 0--1 programs can be found via simple greedy algorithms.
</div>
<div id="bib_han2019fractional" class="bibtex noshow">
<pre>
@article{han2019fractional,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s and Prokopyev, Oleg A},
  title = {Fractional 0-1 programming and submodularity},
  journal = {arXiv preprint arXiv:2012.07235},
  year = {2019},
  url = {https://arxiv.org/abs/2012.07235}
}
</pre>
</div>
</li>
<!-- Item: atamturk2021sparse -->
<li ><p>
Alper Atamt&uuml;rk, Andres Gomez and Shaoning Han,
&ldquo;Sparse and smooth signal estimation: Convexification of l0-formulations,&rdquo;
<i>Journal of Machine Learning Research</i>,
vol. 22,
no. 52,
pp. 1-43,

2021.

<br />
<a href="javascript:toggleAbstract('atamturk2021sparse')">[abstract]</a>
<a href="javascript:toggleBibtex('atamturk2021sparse')">[bibtex]</a>
<a href="https://www.jmlr.org/papers/volume22/18-745/18-745.pdf">[url]</a>




</p>
<div id="abstract_atamturk2021sparse" class="abstract">
Signal estimation problems with smoothness and sparsity priors can be naturally modeled as quadratic optimization with l_0-``norm" constraints. Since such problems are non-convex and hard-to-solve, the standard approach is, instead, to tackle their convex surrogates based on l_1-norm relaxations. In this paper, we propose new iterative conic quadratic relaxations that exploit not only the l_0-``norm" terms, but also the fitness and smoothness functions. The iterative convexification approach substantially closes the gap between the l_0-``norm" and its l_1 surrogate. Experiments using an off-the-shelf conic quadratic solver on synthetic as well as real datasets indicate that the proposed iterative convex relaxations lead to significantly better estimators than l_1-norm while preserving the computational efficiency. In addition, the parameters of the model and the resulting estimators are easily interpretable.
</div>
<div id="bib_atamturk2021sparse" class="bibtex noshow">
<pre>
@article{atamturk2021sparse,
  author = {Atamt&uuml;rk, Alper and Gomez, Andres and Han, Shaoning},
  title = {Sparse and smooth signal estimation: Convexification of l0-formulations},
  journal = {Journal of Machine Learning Research},
  year = {2021},
  volume = {22},
  number = {52},
  pages = {1--43},
  url = {https://www.jmlr.org/papers/volume22/18-745/18-745.pdf}
}
</pre>
</div>
</li>
</ol>
=======
<!-- Generated from JabRef by PubList by Truong Nghiem at 09:32 on 2021.06.18. -->
<ol class="biblist">
<!-- Item: han2021single -->
<li ><p>
Shaoning Han and Andr&eacute;s G&oacute;mez,
&ldquo;Single-neuron convexifications for binarized neural networks,&rdquo;





2021.

<br />
<a href="javascript:toggleAbstract('han2021single')">[abstract]</a>
<a href="javascript:toggleBibtex('han2021single')">[bibtex]</a>
<a href="http://www.optimization-online.org/DB_HTML/2021/05/8419.html">[url]</a>




</p>
<div id="abstract_han2021single" class="abstract">
Binarized neural networks are an important class of neural network in deep learning due to their computational efficiency. This paper contributes towards a better understanding of the structure of binarized neural networks, specifically, ideal convex representations of the activation functions used. We describe the convex hull of the graph of the signum activation function associated with a single neuron, deriving closed forms for the convex and concave envelopes that improve upon those used in the literature. The new formulations lead to improved methods to verify the robustness of a binarized neural network via convex optimization.
</div>
<div id="bib_han2021single" class="bibtex noshow">
<pre>
@article{han2021single,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s},
  title = {Single-neuron convexifications for binarized neural networks},
  year = {2021},
  url = {http://www.optimization-online.org/DB_HTML/2021/05/8419.html}
}
</pre>
</div>
</li>
<!-- Item: han20202x2 -->
<li ><p>
Shaoning Han, Andr&eacute;s G&oacute;mez and Alper Atamt&uuml;rk,
&ldquo;2x2 convexifications for convex quadratic optimization with indicator variables,&rdquo;
<i>arXiv preprint arXiv:2004.07448</i>,




2020.

<br />
<a href="javascript:toggleAbstract('han20202x2')">[abstract]</a>
<a href="javascript:toggleBibtex('han20202x2')">[bibtex]</a>
<a href="https://arxiv.org/abs/2004.07448">[url]</a>




</p>
<div id="abstract_han20202x2" class="abstract">
In this paper, we study the convex quadratic optimization problem with indicator variables. For the bivariate case, we describe the convex hull of the epigraph in the original space of variables, and also give a conic quadratic extended formulation. Then, using the convex hull description for the bivariate case as a building block, we derive an extended SDP relaxation for the general case. This new formulation is stronger than other SDP relaxations proposed in the literature for the problem, including Shor's SDP relaxation, the optimal perspective relaxation as well as the optimal rank-one relaxation. Computational experiments indicate that the proposed formulations are quite effective in reducing the integrality gap of the optimization problems.
</div>
<div id="bib_han20202x2" class="bibtex noshow">
<pre>
@article{han20202x2,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s and Atamt&uuml;rk, Alper},
  title = {2x2 convexifications for convex quadratic optimization with indicator variables},
  journal = {arXiv preprint arXiv:2004.07448},
  year = {2020},
  url = {https://arxiv.org/abs/2004.07448}
}
</pre>
</div>
</li>
<!-- Item: han2019fractional -->
<li ><p>
Shaoning Han, Andr&eacute;s G&oacute;mez and Oleg A. Prokopyev,
&ldquo;Fractional 0-1 programming and submodularity,&rdquo;
<i>arXiv preprint arXiv:2012.07235</i>,




2019.

<br />
<a href="javascript:toggleAbstract('han2019fractional')">[abstract]</a>
<a href="javascript:toggleBibtex('han2019fractional')">[bibtex]</a>
<a href="https://arxiv.org/abs/2012.07235">[url]</a>




</p>
<div id="abstract_han2019fractional" class="abstract">
In this note we study multiple-ratio fractional 0--1 programs, a broad class of NP-hard combinatorial optimization problems. In particular, under some relatively mild assumptions we provide a complete characterization of the conditions, which ensure that a single-ratio function is submodular. Then we illustrate our theoretical results with the assortment optimization and facility location problems, and discuss practical situations that guarantee submodularity in the considered application settings. In such cases, near-optimal solutions for multiple-ratio fractional 0--1 programs can be found via simple greedy algorithms.
</div>
<div id="bib_han2019fractional" class="bibtex noshow">
<pre>
@article{han2019fractional,
  author = {Han, Shaoning and G&oacute;mez, Andr&eacute;s and Prokopyev, Oleg A},
  title = {Fractional 0-1 programming and submodularity},
  journal = {arXiv preprint arXiv:2012.07235},
  year = {2019},
  url = {https://arxiv.org/abs/2012.07235}
}
</pre>
</div>
</li>
<!-- Item: atamturk2021sparse -->
<li ><p>
Alper Atamt&uuml;rk, Andres Gomez and Shaoning Han,
&ldquo;Sparse and smooth signal estimation: Convexification of l0-formulations,&rdquo;
<i>Journal of Machine Learning Research</i>,
vol. 22,
no. 52,
pp. 1-43,

2021.

<br />
<a href="javascript:toggleAbstract('atamturk2021sparse')">[abstract]</a>
<a href="javascript:toggleBibtex('atamturk2021sparse')">[bibtex]</a>
<a href="https://www.jmlr.org/papers/volume22/18-745/18-745.pdf">[url]</a>




</p>
<div id="abstract_atamturk2021sparse" class="abstract">
Signal estimation problems with smoothness and sparsity priors can be naturally modeled as quadratic optimization with l_0-``norm" constraints. Since such problems are non-convex and hard-to-solve, the standard approach is, instead, to tackle their convex surrogates based on l_1-norm relaxations. In this paper, we propose new iterative conic quadratic relaxations that exploit not only the l_0-``norm" terms, but also the fitness and smoothness functions. The iterative convexification approach substantially closes the gap between the l_0-``norm" and its l_1 surrogate. Experiments using an off-the-shelf conic quadratic solver on synthetic as well as real datasets indicate that the proposed iterative convex relaxations lead to significantly better estimators than l_1-norm while preserving the computational efficiency. In addition, the parameters of the model and the resulting estimators are easily interpretable.
</div>
<div id="bib_atamturk2021sparse" class="bibtex noshow">
<pre>
@article{atamturk2021sparse,
  author = {Atamt&uuml;rk, Alper and Gomez, Andres and Han, Shaoning},
  title = {Sparse and smooth signal estimation: Convexification of l0-formulations},
  journal = {Journal of Machine Learning Research},
  year = {2021},
  volume = {22},
  number = {52},
  pages = {1--43},
  url = {https://www.jmlr.org/papers/volume22/18-745/18-745.pdf}
}
</pre>
</div>
</li>
</ol>
>>>>>>> 16795d1559265f016e16aa456e657c85915b8abd
