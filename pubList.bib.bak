% Encoding: ISO-8859-1

@Article{pang2021,
  author    = {Pang, Jong-Shi and Han, Shaning},
  journal   = {Submitted to SIAM Journal on Optimization},
  title     = {Some Strongly Polynomially Solvable Convex Quadratic Programs with Bounded Variables},
  year      = {2021},
  abstract  = {This paper begins with a class of convex quadratic programs (QPs) with bounded variables solvable by the parametric principal pivoting algorithm with $\mathcal{O}(n^3)$ strongly polynomial complexity, where $n$ is the number of variables of the
problem. Extension of the Hessian class is also discussed. Our research is motivated by a preprint [7] wherein the efficient solution of a quadratic program with a tridiagonal Hessian matrix in the quadratic objective is needed for the construction of a polynomial-time algorithm for solving an associated sparse variable selection problem. With the tridiagonal structure, the complexity of the QP algorithm reduces to $\mathcal{O}(n^2)$. Our strongly polynomiality results extend previous works of some strongly
polynomially solvable linear complementarity problems with a P-matrix [9]; special cases of the extended results include weakly quasi-diagonally dominant problems in addition to the tridiagonal ones.},
  timestamp = {202112},
  url       = {http://www.optimization-online.org/DB_HTML/2021/12/8713.html},
}

@Article{han2021equiv,
  author    = {Han, Shaoning and G{\'o}mez, Andr{\'e}s and Atamt{\"u}rk, Alper},
  journal   = {Submitted to Operations Research Letters},
  title     = {The equivalence of optimal perspective formulation and Shor's SDP for quadratic programs with indicator variables},
  year      = {2021},
  abstract  = {In this paper, we compare the strength of the optimal perspective reformulation and Shor's SDP relaxation. We prove these two formulations are equivalent for quadratic optimization problems with indicator variables.},
  timestamp = {202111},
}

@Article{han2021compact,
  author    = {Han, Shaoning and G{\'o}mez, Andr{\'e}s},
  journal   = {Submitted to Mathematics of Operations Research},
  title     = {Compact extended formulations for low-rank functions with indicator variables},
  year      = {2021},
  abstract  = {We study the mixed-integer epigraph of a low-rank convex function with non-convex indicator constraints, which are often used to impose logical constraints on the support of the solutions. Extended formulations describing the convex hull of such sets can easily be constructed via disjunctive programming, although a direct application of this method often yields prohibitively large formulations, whose size is exponential in the number of variables. In this paper, we propose a new disjunctive representation of the sets under study, which leads to compact formulations with size exponential in the rank of the function, but polynomial in the number of variables. Moreover, we show how to project out the additional variables for the case of rank-one functions, recovering or generalizing known results for the convex hulls of such sets (in the original space of variables).},
  timestamp = {202110},
  url       = {https://arxiv.org/abs/2110.14884},
}

@Article{he2021comparing,
  author    = {He, Ziyu and Han, Shaoning and G{\'o}mez, Andr{\'e}s and Cui, Ying and Pang, Jong-Shi},
  journal   = {Submitted to Mathematical Programming},
  title     = {Comparing solution paths of sparse quadratic minimization with a {S}tieltjes matrix},
  year      = {2021},
  abstract  = {This paper studies several solution paths of sparse quadratic minimization problems as a function of the weighing parameter of the bi-objective of estimation loss versus solution sparsity. Three such paths are considered: the "L0-path" where the discontinuous L0-function provides the exact sparsity count; the "L1-path" where the L1-function provides a convex surrogate of sparsity count; and the "capped L1-path" where the nonconvex nondifferentiable capped L1-function aims to enhance the L1-approximation. Serving different purposes, each of these three formulations is different from each other, both analytically and computationally. Our results deepen the understanding of (old and new)properties of the associated paths, highlight the pros, cons, and tradeoffs of these sparse optimization models, and provide numerical evidence to support the practical superiority of the capped L1-path. Our study of the capped L1-path is the first of its kind as the path pertains to computable directionally stationary (= strongly locally minimizing in this context, as opposed to globally optimal) solutions of a parametric nonconvex nondifferentiable optimization problem. Motivated by classical parametric quadratic programming theory and reinforced by modern statistical learning studies, both casting an exponential perspective in fully describing such solution paths, we also aim to address the question of whether some of them can be fully traced in strongly polynomial time in the problem dimensions. A major conclusion of this paper is that a path of directional stationary solutions of the capped L1-regularized problem offers interesting theoretical properties and practical compromise between the L0-path and the L1-path. Indeed, while the L0-path is computationally prohibitive and greatly handicapped by the repeated solution of mixed integer nonlinear programs, the quality of L1-path, in terms of the two criteria---loss and sparsity--- in the estimation objective, is inferior to the capped L1-path; the latter can be obtained efficiently by a combination of a parametric pivoting-like scheme supplemented by an algorithm that takes advantage of the Z-matrix structure of the loss function.},
  timestamp = {202110},
  url       = {http://www.optimization-online.org/DB_HTML/2021/09/8608.html},
}

@Article{han2021single,
  author    = {Han, Shaoning and G{\'o}mez, Andr{\'e}s},
  journal   = {In preparation},
  title     = {Single-neuron convexifications for binarized neural networks},
  year      = {2021},
  abstract  = {Binarized neural networks are an important class of neural network in deep learning due to their computational efficiency. This paper contributes towards a better understanding of the structure of binarized neural networks, specifically, ideal convex representations of the activation functions used. We describe the convex hull of the graph of the signum activation function associated with a single neuron, deriving closed forms for the convex and concave envelopes that improve upon those used in the literature. The new formulations lead to improved methods to verify the robustness of a binarized neural network via convex optimization.},
  timestamp = {202105},
  url       = {http://www.optimization-online.org/DB_HTML/2021/05/8419.html},
}

@Article{han20202x2,
  author    = {Han, Shaoning and G{\'o}mez, Andr{\'e}s and Atamt{\"u}rk, Alper},
  journal   = {Mathematical Programming, under major revision},
  title     = {2x2 convexifications for convex quadratic optimization with indicator variables},
  year      = {2020},
  abstract  = {In this paper, we study the convex quadratic optimization problem with indicator variables. For the bivariate case, we describe the convex hull of the epigraph in the original space of variables, and also give a conic quadratic extended formulation. Then, using the convex hull description for the bivariate case as a building block, we derive an extended SDP relaxation for the general case. This new formulation is stronger than other SDP relaxations proposed in the literature for the problem, including Shor's SDP relaxation, the optimal perspective relaxation as well as the optimal rank-one relaxation. Computational experiments indicate that the proposed formulations are quite effective in reducing the integrality gap of the optimization problems.},
  slide     = {slides/informs2020.pdf},
  timestamp = {2020},
  url       = {https://arxiv.org/abs/2004.07448},
}

@Article{han2019fractional,
  author    = {Han, Shaoning and G{\'o}mez, Andr{\'e}s and Prokopyev, Oleg A},
  journal   = {Journal of Global Optimization, conditionally accepted},
  title     = {Fractional 0-1 programming and submodularity},
  year      = {2019},
  abstract  = {In this note we study multiple-ratio fractional 0--1 programs, a broad class of NP-hard combinatorial optimization problems. In particular, under some relatively mild assumptions we provide a complete characterization of the conditions, which ensure that a single-ratio function is submodular. Then we illustrate our theoretical results with the assortment optimization and facility location problems, and discuss practical situations that guarantee submodularity in the considered application settings. In such cases, near-optimal solutions for multiple-ratio fractional 0--1 programs can be found via simple greedy algorithms.},
  slide     = {slides/informs2021.pdf},
  timestamp = {2019},
  url       = {https://arxiv.org/abs/2012.07235},
}

@Article{atamturk2021sparse,
  author    = {Atamt{\"u}rk, Alper and G{\'o}mez, Andr{\'e}s and Han, Shaoning},
  journal   = {Journal of Machine Learning Research},
  title     = {Sparse and smooth signal estimation: Convexification of l0-formulations},
  year      = {2021},
  number    = {52},
  pages     = {1--43},
  volume    = {22},
  abstract  = {Signal estimation problems with smoothness and sparsity priors can be naturally modeled as quadratic optimization with l_0-``norm" constraints. Since such problems are non-convex and hard-to-solve, the standard approach is, instead, to tackle their convex surrogates based on l_1-norm relaxations. In this paper, we propose new iterative conic quadratic relaxations that exploit not only the l_0-``norm" terms, but also the fitness and smoothness functions. The iterative convexification approach substantially closes the gap between the l_0-``norm" and its l_1 surrogate. Experiments using an off-the-shelf conic quadratic solver on synthetic as well as real datasets indicate that the proposed iterative convex relaxations lead to significantly better estimators than l_1-norm while preserving the computational efficiency. In addition, the parameters of the model and the resulting estimators are easily interpretable.},
  timestamp = {2018},
  url       = {https://www.jmlr.org/papers/volume22/18-745/18-745.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: saveOrderConfig:specified;timestamp;true;year;false;title;false;}
